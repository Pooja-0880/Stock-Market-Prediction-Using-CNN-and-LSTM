{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d3f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dense, GlobalMaxPooling1D, Embedding\n",
    "\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe5cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aapl = pd.read_csv(\"stock_datasets/df_AAPL.csv\")\n",
    "# df_googl = pd.read_csv(\"stock_datasets/df_stocktwits_googl.csv\")\n",
    "# df_ma = pd.read_csv(\"stock_datasets/df_stocktwits_ma.csv\")\n",
    "# df_amzn = pd.read_csv(\"stock_datasets/df_stocktwits_amzn.csv\")\n",
    "# df_jnj = pd.read_csv(\"stock_datasets/df_stocktwits_jnj.csv\")\n",
    "\n",
    "def clean_columns(df):\n",
    "    \n",
    "     # remove extra column \n",
    "    df.drop('Unnamed: 0', axis=1, inplace = True)  \n",
    "    df['Text_Cleaned'].replace(\"[^a-zA-Z]\",\" \", regex=True, inplace=True) \n",
    "    df['Text_Cleaned'].replace(\"[\\d.]\", \"\", regex=True, inplace=True)\n",
    "    df['Text_Cleaned'] = df['Text_Cleaned'].str.strip()\n",
    "    return df\n",
    "# df_list = [df_aapl]#, df_googl,df_ma, df_amzn, df_jnj]\n",
    "\n",
    "# for df in df_list: \n",
    "#     df= clean_columns(df)\n",
    "\n",
    "df_aapl = clean_columns(df_aapl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c675cef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-11-15</td>\n",
       "      <td>RT howardlindzon: Looks like Goldman $gs  is t...</td>\n",
       "      <td>rt howardlindzon looks like goldman gs is tryi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-11-15</td>\n",
       "      <td>$AAPL http://stks.co/17zl (Weekly Chart) Appro...</td>\n",
       "      <td>aapl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-11-15</td>\n",
       "      <td>$AAPL down -8.26% this morning? That is a real...</td>\n",
       "      <td>aapl down negative      percent this morning t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-11-15</td>\n",
       "      <td>RT Zguy: $AAPL down -8.26% this morning? That ...</td>\n",
       "      <td>rt zguy aapl down negative      percent this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-11-15</td>\n",
       "      <td>NEW POST: FROZEN TURKEYS http://stks.co/181s $...</td>\n",
       "      <td>new post frozen turkeys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190970</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>Dan Niles: In summary, my 2 overarching invest...</td>\n",
       "      <td>dan niles in summary my   overarching investme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190971</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>$AAPL</td>\n",
       "      <td>aapl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190972</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>$AAPL $MSFT $GOOGL $AMZN\\nI will buy more and ...</td>\n",
       "      <td>aapl msft googl amzn i will buy more and sleep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190973</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>$AAPL bye apple, hello meta</td>\n",
       "      <td>aapl bye apple hello meta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190974</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>$AAPL still don&amp;#39;t have a reason to invest ...</td>\n",
       "      <td>aapl still don and t have a reason to invest h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190975 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date_time                                              tweet  \\\n",
       "0       2011-11-15  RT howardlindzon: Looks like Goldman $gs  is t...   \n",
       "1       2011-11-15  $AAPL http://stks.co/17zl (Weekly Chart) Appro...   \n",
       "2       2011-11-15  $AAPL down -8.26% this morning? That is a real...   \n",
       "3       2011-11-15  RT Zguy: $AAPL down -8.26% this morning? That ...   \n",
       "4       2011-11-15  NEW POST: FROZEN TURKEYS http://stks.co/181s $...   \n",
       "...            ...                                                ...   \n",
       "190970  2023-01-04  Dan Niles: In summary, my 2 overarching invest...   \n",
       "190971  2023-01-04                                              $AAPL   \n",
       "190972  2023-01-04  $AAPL $MSFT $GOOGL $AMZN\\nI will buy more and ...   \n",
       "190973  2023-01-04                        $AAPL bye apple, hello meta   \n",
       "190974  2023-01-04  $AAPL still don&#39;t have a reason to invest ...   \n",
       "\n",
       "                                             Text_Cleaned  \n",
       "0       rt howardlindzon looks like goldman gs is tryi...  \n",
       "1                                                    aapl  \n",
       "2       aapl down negative      percent this morning t...  \n",
       "3       rt zguy aapl down negative      percent this m...  \n",
       "4                                 new post frozen turkeys  \n",
       "...                                                   ...  \n",
       "190970  dan niles in summary my   overarching investme...  \n",
       "190971                                               aapl  \n",
       "190972  aapl msft googl amzn i will buy more and sleep...  \n",
       "190973                          aapl bye apple hello meta  \n",
       "190974  aapl still don and t have a reason to invest h...  \n",
       "\n",
       "[190975 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aapl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b44c3161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preparing vader sentiment analyser \n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "f = lambda title: vader.polarity_scores(title)['compound']\n",
    "df_aapl['compound'] = df_aapl['tweet'].apply(f)\n",
    "#df_aapl['date_time'] = pd.to_datetime(df_aapl.date_time).dt.date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0516f939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,10))\n",
    "# mean_df = df_aapl.groupby([ 'date_time']).mean().unstack()\n",
    "# mean_df = mean_df.xs('compound')\n",
    "# mean_df.plot(kind='bar')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fe1e81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df train shape :  (127953,)\n",
      "df test shape :  (63022,)\n",
      "y train shape :  (127953,)\n",
      "y test shape :  (63022,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"sentiment\": df_aapl['compound'], \n",
    "                   \"data\": df_aapl['Text_Cleaned']})\n",
    "df['data'] = df['data'].astype(str)\n",
    "df['data'] = df['data'].str.strip()\n",
    "\n",
    "df_train, df_test, y_train, y_test = train_test_split(df['data'],df['sentiment'],test_size=0.33,random_state=42)\n",
    "\n",
    "print(\"df train shape : \", df_train.shape)\n",
    "print(\"df test shape : \", df_test.shape)\n",
    "print(\"y train shape : \", y_train.shape)\n",
    "print(\"y test shape : \", y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ebc8c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Building deep learning model \n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"stock_datasets/df_AAPL.csv\") # if this fails then run \"python -m spacy download en_core_web_lg\" to download that model\n",
    "\n",
    "\n",
    "max_words = 1000\n",
    "tokenizer=Tokenizer(max_words)\n",
    "tokenizer.fit_on_texts(df_train)\n",
    "sequence_train=tokenizer.texts_to_sequences(df_train)\n",
    "sequence_test=tokenizer.texts_to_sequences(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f2a4d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 38806 number of independent tokens \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(127953, 309)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec = tokenizer.word_index\n",
    "V = len(word2vec)\n",
    "print('dataset has %s number of independent tokens '%V)\n",
    "\n",
    "data_train = pad_sequences(sequence_train)\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c53826b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63022, 309)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = data_train.shape[1]\n",
    "data_test = pad_sequences(sequence_test, maxlen=T)\n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88479a43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 309)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 309, 20)           776140    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 307, 32)           1952      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 102, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 100, 64)           6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 33, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 31, 128)           24704     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 809,649\n",
      "Trainable params: 809,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "D = 20 \n",
    "i=Input((T,))\n",
    "x=Embedding(V+1,D)(i)\n",
    "x=Conv1D(32,3,activation='relu')(x)\n",
    "x=MaxPooling1D(3)(x)\n",
    "x=Conv1D(64,3,activation='relu')(x)\n",
    "x=MaxPooling1D(3)(x)\n",
    "x=Conv1D(128,3,activation='relu')(x)\n",
    "x=GlobalMaxPooling1D()(x)\n",
    "x=Dense(5,activation='softmax')(x)\n",
    "model=Model(i,x)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f5455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127953 samples, validate on 63022 samples\n",
      "Epoch 1/50\n",
      "127953/127953 [==============================] - 94s 734us/sample - loss: 0.0153 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 2/50\n",
      "127953/127953 [==============================] - 89s 699us/sample - loss: 4.1925e-11 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 3/50\n",
      "127953/127953 [==============================] - 89s 698us/sample - loss: 4.1925e-11 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 4/50\n",
      "127953/127953 [==============================] - 89s 698us/sample - loss: 4.1925e-11 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 5/50\n",
      "127953/127953 [==============================] - 88s 688us/sample - loss: 3.9130e-11 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 6/50\n",
      "127953/127953 [==============================] - 90s 703us/sample - loss: 3.6335e-11 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 7/50\n",
      "127953/127953 [==============================] - 86s 672us/sample - loss: 3.2608e-11 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 8/50\n",
      "127953/127953 [==============================] - 88s 686us/sample - loss: 2.7018e-11 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 9/50\n",
      "127953/127953 [==============================] - 88s 689us/sample - loss: 2.1428e-11 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 10/50\n",
      "127953/127953 [==============================] - 88s 685us/sample - loss: 1.6770e-11 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 11/50\n",
      "127953/127953 [==============================] - 87s 683us/sample - loss: 9.3166e-12 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 12/50\n",
      "127953/127953 [==============================] - 86s 669us/sample - loss: 8.3850e-12 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 13/50\n",
      "127953/127953 [==============================] - 89s 698us/sample - loss: 5.5900e-12 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 14/50\n",
      "127953/127953 [==============================] - 86s 670us/sample - loss: 3.7267e-12 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 15/50\n",
      "127953/127953 [==============================] - 87s 681us/sample - loss: 3.7267e-12 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 16/50\n",
      "127953/127953 [==============================] - 91s 708us/sample - loss: 2.7950e-12 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 17/50\n",
      "127953/127953 [==============================] - 91s 711us/sample - loss: 2.7950e-12 - accuracy: 0.4255 - val_loss: 0.0000e+00 - val_accuracy: 0.4279\n",
      "Epoch 18/50\n",
      " 43400/127953 [=========>....................] - ETA: 54s - loss: 0.0000e+00 - accuracy: 0.4262"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "data_train = np.asarray(data_train)\n",
    "y_train = np.asarray(y_train)\n",
    "data_test = np.asarray(data_test)\n",
    "y_test = np.asarray(y_test)\n",
    "cnn_senti=model.fit(data_train,y_train,validation_data=(data_test,y_test),epochs=50,batch_size=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e51cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(data_test)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.argmax(y_pred,axis=1)\n",
    "#y_pred.astype(float)\n",
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a234474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083b2870",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f331ab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,y_pred)\n",
    "ax=sns.heatmap(cm,annot=True,cmap='Blues',fmt=' ')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('y_test')\n",
    "ax.set_ylabel('y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155339d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
